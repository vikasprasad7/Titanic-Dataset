import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

df = pd.read_csv('D:/Downloads/titanic/train.csv')
df.head(10)
df.isnull().sum()
df['Age'].fillna(df['Age'].mean(), inplace=True)
df['Embarked'].fillna('S', inplace=True)
sex_class = {'male':1,'female':0}
df['Sex'] = df['Sex'].map(sex_class)
embarked = {'S':1,'C':2,'Q':3}
df['Embarked'] = df['Embarked'].map(embarked)
df.head()
#Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
X = df[['Pclass','Age','Parch','Fare','Sex','Embarked','SibSp','Family Size']]
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
Accuracy: 0.81
#Naive Bayes
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import GaussianNB#MultinomialNB
X = df[['Pclass', 'Age','SibSp','Sex','Parch','Fare','Embarked','Family Size']]
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
Accuracy: 0.74
#MultinomialNB
X = df[['Pclass', 'Age','SibSp','Sex','Parch','Fare','Embarked','Family Size']]#Negative Values not accepted
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)
mnb = MultinomialNB()
mnb.fit(X_train, y_train)
y_pred = mnb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
Accuracy: 0.68
BernoulliNB
X = df[['Pclass', 'Age','SibSp','Sex','Parch','Fare','Embarked','Family Size','EmFam','EmFamMi','PcEmb','PcEmbMi','PcEmbMu','EP']]
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)
bnb = BernoulliNB()
bnb.fit(X_train, y_train)
y_pred = bnb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
Accuracy: 0.69
#SVC
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
X = df[['Pclass', 'Age', 'SibSp', 'Sex', 'Parch', 'Fare', 'Embarked','Family Size']]
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)
svc = SVC(kernel='linear', random_state=44)#look for other options for kernel
svc.fit(X_train, y_train)
y_pred = svc.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
Accuracy: 0.73
#Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt
X = df[['Pclass','Age','SibSp','Sex','Parch','Fare','Embarked','Family Size','EmFam','EmFamMi','PcEmb','PcEmbMi','PcEmbMu','EP']]
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)
clf = DecisionTreeClassifier(random_state=34)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = clf.score(X_test, y_test)
print(f'Accuracy: {accuracy:.2f}')
Accuracy: 0.70
from sklearn.ensemble import RandomForestClassifier
X = df[['Pclass', 'Age', 'SibSp', 'Sex', 'Parch', 'Fare', 'Embarked','Family Size','EmFam','EmFamMi','PcEmb','PcEmbMi','PcEmbMu','EP']]
y = df['Survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)
y_pred = rf_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')
